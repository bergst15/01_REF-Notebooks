{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Reference Noteboook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "Paper by Hadley Wickham <br>\n",
    "https://vita.had.co.nz/papers/tidy-data.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Resampling\n",
    "\n",
    "When using aggregates like `mean()`, `sum()`, `count()`, etc. pandas can be manipulated to adjust the sampling frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'D' indicates daily\n",
    "daily_mean = sales.resample('D').mean()\n",
    "\n",
    "# '2W' indicates bi-weekly\n",
    "weekly = sales.resample('2W').mean()\n",
    "\n",
    "# 'B' indicates per business day, ffill() forward fills\n",
    "bdaily_mean = sales.resample('B').mean().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling/Moving Average\n",
    "\n",
    "The primary purpose is to smooth out short term fluctuations. To use the `.rolling()` method, you must always use method chaining, first calling `.rolling()` and then chaining an aggregation method after it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data.rolling(window=24).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values\n",
    "\n",
    "Missing values are trouble for machine learning. Fill them in with pandas. Note that you don't need to pass an argumen to `impute_median` below since you are calling it from `.transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "titanic.age = titanic.groupby(['sex'.'pclass'].age.transform(impute_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Filtering\n",
    "\n",
    "Complex example that is probably super useful."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DF named 'auto'\n",
    "   mpg   cyl displ   hp   weight  accel yr   origin    name\n",
    "0  18.0    8  307.0  130    3504   12.0  70     US    chevrolet malibu\n",
    "1  15.0    8  350.0  165    3693   11.5  70     US    buick skylark\n",
    "2  18.0    8  318.0  150    3436   11.0  70     US    plymouth satellite\n",
    "3  16.0    8  304.0  150    3433   12.0  70     US    amc rebel sst\n",
    "4  17.0    8  302.0  140    3449   10.5  70     US    ford torino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting = auto.groupby('yr')\n",
    "\n",
    "type(splitting)\n",
    "'pandas.core.groupby.DataFrameGroupBy'\n",
    "type(splitting.groups)\n",
    "'dict'\n",
    "\n",
    "print(splitting.groups.keys())\n",
    "'dict_keys([70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])'\n",
    "\n",
    "# groupby object: iteration\n",
    "for group_name, group in splitting:\n",
    "    avg = group['mpg'].mean()\n",
    "    print(group_name, avg)\n",
    "\n",
    "# groupby object: iteration and filtering\n",
    "for group_name, group in splitting:\n",
    "    avg = group.loc[group['name'].str.contains('chevrolet'), 'mpg'].mean()\n",
    "    print(group_name, avg)\n",
    "    \n",
    "# groupby object: comprehension\n",
    "chevy_means = {year:group\n",
    "               .loc[group['name'].str.contains('chevrolet'),'mpg'].mean()\n",
    "               for year,group in splitting}\n",
    "pd.Series(chevy_means)\n",
    "\n",
    "# boolean groupby\n",
    "chevy = auto['name'].str.contains('chevrolet')\n",
    "auto.groupby(['yr', chevy])['mpg'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and filtering with .filter()\n",
    "\n",
    "You can use groupby with the .filter() method to remove whole groups of rows from a DataFrame based on a boolean condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "by_company = sales.groupby('Company')\n",
    "by_com_sum = by_company.Units.sum()\n",
    "by_com_filt = by_company.filter(lambda g: g.Units.sum() > 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and filtering with .map()\n",
    "\n",
    "You may instead want to group by a function/transformation of a column. The key here is that the Series is indexed the same way as the DataFrame. You can also mix and match column grouping with Series grouping. Here, calling `.mean()` on the survived column returns the percentage that survived as survival is indicated by a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under10 = titanic.age < 10\n",
    "under10 = under10.map({True:'under 10',False:'over 10'})\n",
    "\n",
    "# grouped by only under 10\n",
    "survived_mean_1 = titanic.groupby(under10).survived.mean()\n",
    "\n",
    "# grouped by under 10 and pclass\n",
    "survived_mean_2 = titanic.groupby([under10,'pclass']).survived.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .idxmax() and .idxmin() methods\n",
    "\n",
    "These two methods return the index of the max or min value respectively. This also can be used on columns with `df.T.idxmax(axis='columns')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .nunique() method\n",
    "\n",
    "Given a categorical Series `S`, `S.nunique()` returns the number of distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Example 1\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "\n",
    "# Example 2\n",
    "df = pd.read_sql_query(\n",
    "    'SELECT * FROM Employee WHERE EmployeeID >= 6 ORDER BY BirthDate'\n",
    "    ,engine)\n",
    "\n",
    "# Example 3 (inner join)\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT Title, Name FROM Album INNER JOIN Artist on \n",
    "    Album.ArtistID = Artist.ArtistID\"\"\"\n",
    "    ,engine)\n",
    "\n",
    "# Example 4 (inner join with filtering)\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM PlaylistTrack INNER JOIN Track ON \n",
    "    PlaylistTrack.TrackID = Track.TrackID \n",
    "    WHERE Milliseconds < 250000\n",
    "    \"\"\"\n",
    "    ,engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyODBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# set up dsn, uid, pwd through local config utility\n",
    "\n",
    "sql = ''' SQL HERE '''\n",
    "\n",
    "with pyodbc.connect(f'DSN={‘dsn’}; UID={‘uid’}; PWD={‘pwd’}') as connection:\n",
    "    df = pd.read_sql(sql, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehension within a DataFrame\n",
    "\n",
    "Here we use list comprehension to populate a new column in a DataFrame. The two values of each tuple are multiplied and the result is added to new column `'Total Urban Population`'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example only, cell will not run\n",
    "pops = zip(df_pop_ceb['Total Population'], \n",
    "           df_pop_ceb['Urban population (% of total)'])\n",
    "pops_list = list(pops)\n",
    "\n",
    "df_pop_ceb['Total Urban Population'] = [int(p1*p2*0.01) for p1,p2 in pops_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing characters in column names\n",
    "\n",
    "Here we use `.str.replace()` to replace a single character within a set of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_c.columns = temps_f.columns.str.replace('F','C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a Column, .split() and .get()\n",
    "\n",
    "Another common way multiple variables are stored in columns is with a delimiter. First split the string using `.split()`, then retrieve the first element with `.get(0)` and the second element with `.get(1)`. <br>\n",
    "\n",
    "Remember to access the `.str` attribute before applying `.split()` or `.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting a column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# retrieving first word\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# retrieving second word\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globbing (glob)\n",
    "\n",
    "The `glob` module can be used to match patterns and return a list. This is useful if you need to load multiple data files in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "pattern = '*.csv' # * multi, ? single\n",
    "csv_files = glob.glob(pattern)\n",
    "frames = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(csv)\n",
    "    frames.append(df)\n",
    "    \n",
    "uber = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (RegEx)\n",
    "\n",
    "Cheat sheet: <br>\n",
    "https://www.rexegg.com/regex-quickstart.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17        \\d*   \n",
    "$17       \\$\\d*\n",
    "$17.00    \\$\\d*\\.\\d*\n",
    "$17.89    \\$\\d*\\.\\d{2}\n",
    "$17.895   ^\\$\\d*\\.\\d{2}$  # ^ and $ force exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# example 1\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')\n",
    "bool(result)\n",
    "\n",
    "# example 2\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "result = prog.match('123-456-7890')\n",
    "bool(result)\n",
    "\n",
    "# example 3 (findall())\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Function and RegEx\n",
    "\n",
    "The following function removes the $ dollar sign from the values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips['total_dollar_re'] = \n",
    "    tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with asserts\n",
    "\n",
    "You can use `assert` to programmatically check for missing data vs visually inspection. Note that by chaining two `.all()` methods you will first check each value, then check the results against themselves. <br>\n",
    "<br>\n",
    "Note that `ebola` is a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "assert ebola.notnull().all().all()\n",
    "\n",
    "# check that values are >= 0\n",
    "assert (ebola >= 0).all().all()\n",
    "\n",
    "# check for null\n",
    "assert df.isull().any(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Multiple DataFrames\n",
    "\n",
    "Using a for loop with a list of filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
    "\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a for loop with string interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for medal in medal_types:\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    columns = ['Country', medal]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dictionary of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame from file_path: editions\n",
    "editions = pd.read_csv(file_path,sep='\\t')\n",
    "\n",
    "# Extract the relevant columns: editions\n",
    "editions = editions[['Edition','Grand Total','City','Country']]\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[year][['Athlete','NOC','Medal']]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index=True)\n",
    "\n",
    "# Print first and last 5 rows of medals\n",
    "print(medals.head())\n",
    "print(medals.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames\n",
    "\n",
    "Here are simplified guidelines for deciding which pandas method to use when combining DataFrames (ranked roughly in order of increasing complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df1.append(df2)`\n",
    "- stacking vertically <br>\n",
    "\n",
    "`pd.concat([df1,df2])`\n",
    "- stacking many horizontally or vertically\n",
    "- simple inner/outer joines on indexes  <br>\n",
    "\n",
    "`df1.join(df2)`\n",
    "- inner/outer/left/right joins on indexes <br>\n",
    "\n",
    "`pd.merge([df1,df2])`\n",
    "- many joins on multiple columns <br>\n",
    "\n",
    "`pd.merge_ordered([df1,df2])`\n",
    "- same as merge but for ordered data like time series data <br>\n",
    "\n",
    "`pd.merge_asof([df1,df2])`\n",
    "- similar to `pd.merge_ordered()` except this will also merge values in order using the 'on' column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Operations on DataFrames and Series\n",
    "\n",
    "Series with common indexes can be manipulated with simple arithmetic operations such as `+` and `-` as is. Specific functions and methods may be useful when indexes do not align exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide with `.divide()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_range.divide(week1_mean, axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add with `.add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.Kelvin = temperature.Celsius.add(273.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply with `.multiply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pounds = dollars.multiply(exchange['GBP/USD'],axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent change with `.pct_change()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent change\n",
    "week1_mean.pct_change() * 100 # multiply by 100 to show as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
